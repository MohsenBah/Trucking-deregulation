---
title: "Trucking deregulation"
author: "Mohsen Bahremani"
date: "11/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(ISLR)
library(corrgram)
library(car)
```

```{r}
df<-read.csv('trucking.csv')
set.seed(1) #Ensure reproducible code 
df=df[,-c(6)]
head(df)

```

```{r}
str(df)
summary(df)
summary(is.na(df))

```

Investigate the data types in data frame (df) data structure to ensure no changes is required. Investigate if there are any missing values (NA) in each variables. For this dataset, luckily is quited clean.

```{r}
ggplot(df,aes(x=x1))+geom_histogram(color='darkblue', fill='lightblue') + theme(axis.text.x= element_text(angle=90,hjust=1)) + ggtitle("Distribution of milage")
```

```{r}
ggplot(df,aes(x=x2))+geom_histogram(color='darkblue', fill='lightblue') + theme(axis.text.x= element_text(angle=90,hjust=1)) + ggtitle("Histogram of shipment weight")
```

```{r}
ggplot(data=df, aes(x=x1, y=Y)) + geom_boxplot() +ggtitle('(Boxplot x1)') + theme(axis.text.x= element_text(angle=90,hjust=1))+
geom_hline(aes(yintercept=mean(Y)),color="blue", linetype="dashed",size=1, alpha = 1) + geom_text(aes(2,mean(Y),label = 'mean', vjust = -1),color='blue')
ggplot(data=df, aes(x=x2, y=Y)) + geom_boxplot() +ggtitle('(Boxplot x2)') + theme(axis.text.x= element_text(angle=90,hjust=1))+
geom_hline(aes(yintercept=mean(Y)),color="blue", linetype="dashed",size=1, alpha = 1) + geom_text(aes(2,mean(Y),label = 'mean', vjust = -1),color='blue')

```

Two outliers are indentified, but at this moment we ignore them.

```{r}
#corrgram(df[,c(1,2,3)], order=TRUE, upper.panel=panel.cor)
library(PerformanceAnalytics)
chart.Correlation(df[,c(1,2,3)], histogram = TRUE, pch = 19)

```
There is no collinarity as this dataset, because of the low correlation between x1 and x2. 

## Full model
```{r}
fullmodel=lm(Y~x1+x2+x1*x2+I(x1^2)+I(x2^2)+x3+x4+x3*x4+x1*x3+x1*x4+x1*x3*x4+x2*x3+x2*x4+x2*x3*x4+x1*x2*x3+x1*x2*x4+x1*x2*x3*x4+I(x1^2)*x3+I(x1^2)*x4+I(x1^2)*x3*x4+I(x2^2)*x3+I(x2^2)*x4+I(x2^2)*x3*x4, data=df)
summary(fullmodel)
```

```{r}
anova(fullmodel)
```
Initially, we tried to model "the price charged per ton-mile" at full dimention of the second order model contains quadratic (curvature) terms for quantitative variables and interactions among the quantitative and qualitative terms. 
The R_squared is reasonable (0.932).  

## Model improvement

```{r}
library(leaps)
leaps.ck <- function (X, Y, nbest =3) 
{
  
  leaps.out <- leaps(X,Y,method="Cp",nbest=nbest)
  
  leaps.mat <- cbind(leaps.out$size,round(leaps.out$Cp,digits=4))
  
  colnames(leaps.mat) <- c("k","C_k")
  
  leaps.mat <- cbind(leaps.out$which, leaps.mat)
  
  return(leaps.mat)
  
} 

leaps.ck(df[,c(2,3,4,5)],df$Y)

```
 we tried to check it by Forward feature selection. 
```{r}
result.null <- lm(Y~1, data=df)
step(result.null,scope=list(lower=result.null,upper=fullmodel),direction="forward",verbose=FALSE)
```

```{r}
step(fullmodel,direction="backward",verbose=FALSE)

```

```{r}
step(result.null,scope=list(upper=fullmodel),direction="both",verbose=FALSE)

```

```{r}
model1=lm(Y~  x1 +  x3   +   x2   + I(x1^2)  +  x3*x2 ,data=df)
model2=lm(Y~x1+x2+I(x1^2) +x3 + x4+ x1*x2 +x3*x4+ x1*x3 +  x1*x4 + x2*x3 +  x2*x4 + x1*x3*x4 + x2*x3*x4 + x1*x2*x3 + x1*x2*x4+x1*x2*x3*x4 ,data=df)
summary(model1)
summary(model2)
```

```{r}
anova(model1,fullmodel)
anova(model2,fullmodel)

```
The nested analysis showed us the both of the reduced models are significant than full model. Therefore, we continued with reduced models.

```{r}
### Modified Box-Cox transformation ###
box.cox <- function(x,y,intercept=TRUE, ylim=NULL, 
                   lambda =seq (-2, 2, len=42), transform.x=FALSE,verbose =TRUE, make.plot=TRUE)
# Applies Box-Cox transformation to y with parameter values lambda,
# prints the fitted regressions, makes a plot of the log likelihood versus lambda, and returns a
# vector containing the log likelihood values, normalized for the transformation, for each lambda.
# x - X matrix for current model (no column of 1's should be included for an intercept term);
# y - Y vector. 
# intercept - Set to FALSE for a no-intercept model.
# ylim - The range of log-likelihood values on the y-axis (if NULL, then use range(y)).
#        Useful if you want to use the same scale for two plots. 
# lambda - Y is raised to the power lambda. For log(Y) set lambda=0.
# transform.x - Apply the same transformation to all elements of x?
# verbose - Print fitted regression for each lambda?
# make.plot - Make the plot?

{  # Use only cases where y > 0 - otherwise geometric mean undefined. 
     good.cases <- (y>0)
     y <- y[good.cases]
     x <- as.matrix(x)
     x <- x[good.cases, , drop=F]
     

     # Geometric mean of y.
     g <- exp(mean(log(y)))
  
     if(transform.x)
     { 

        # An x column will only be transformed if all values 
        # are positive. 
        x.pos <- vector(mode= "logical", length=ncol(x))
        for(j in 1:ncol(x))
           x.pos[j] <- (min(x[j]) > 0)
           x.name <- dimnames(x)[[2]]
         if( mode(x.name)=="NULL")
           x.name <- paste("X",1:ncol(x),sep="")
     } 

   log.lik <- vector(mode ="numeric",length=length(lambda))

   for(i in 1:length(lambda))
  {     
     if(lambda[i] !=0)
      {  
         # Don't apply constants. In particular, subtracting
         # 1.0 would introduce intercept in no-intercept model.
         # Normalizations aplied to SS(Res) later.
         z <- y^lambda[i]
         if (transform.x)
       {  
          x.new <- x 
          x.new.name <- x.name
           for(j in 1:ncol(x))
              if(x.pos[j])
              { 
                 x.new[,j] <- x[,j]^lambda[i] 
                 x.new.name[j] <- paste(x.name[j],"^",lambda[i], sep="")
          }

    }

 }

  else
  { 
          z<- log(y)
          if(transform.x)
           { 
             x.new <- x 
             x.new.name <- x.name
              for(j in 1:ncol(x))
                  if(x.pos[j])
                 { 
                     x.new[,j] <- log(x[,j])
                     x.new.name[j] <- paste("log(",x.name[j],")",sep="")
                }
           }

      }

  if(transform.x)
  {
       dimnames(x.new) <- list(NULL,x.new.name)
       reg <- lsfit(x.new,z,intercept=intercept) 
   }
  else 
       reg <- lsfit(x,z,intercept = intercept)
 
  if(verbose)
 { cat("Lambda:",lambda[i], "\n")
   ls.print(reg)
 }

 res <- reg$residuals
 ss.res <- sum(res^2)/g^(2.0*(lambda[i] - 1.0 ))
 if (lambda[i] !=0.0)
  ss.res <- ss.res/lambda[i]^2
 log.lik[i] <- -length(y)/2.0*log(ss.res)
 }

if(make.plot)
{ 
 if(mode(ylim)=="NULL")
   ylim <- range(log.lik)
   plot(lambda,log.lik,ylim=ylim, ylab="Log Likelihood", type="b", lty=1)
   abline(max(log.lik)-3.8416/2,0,lty=2)
 }
 
return(log.lik)

}
result=box.cox(df[1:4],df$Y, verbose = FALSE) # the function returns the log-liklihood values.
lambda_seq = seq (-2, 2,len=42) # This sequence is taken from the function above.
optimal_lambda = lambda_seq[which.max(result)]

```
The respose variable is logarithmic, as it describe in the question. The box-cox algorithm confirm that the variables do not need to any transformation. 

## Model Diagnostics
```{r}
par(mfrow = c(2,2))
plot(df$x1, model1$reiduals , xlab ="X1" , ylab= "Residuals", main = "   Residuals vs x")
plot(df$x2, model1$reiduals , xlab ="X2" , ylab= "Residuals", main = "   Residuals vs x2")
plot(df$x3, model1$reiduals , xlab ="X3" , ylab= "Residuals", main = "   Residuals vs x3")
plot(df$x4, model1$reiduals , xlab ="X4" , ylab= "Residuals", main = "   Residuals vs x4")
```
```{r}
par(mfrow = c(1,2))
plot(model1,1)
plot(model1,2)
```



```{r}
par(mfrow = c(1,2))
plot(model2,1)
plot(model2,2)
```
The model diagnostic plots above show that the model is passable. There is good scatter of the residuals around zero for the range of fitted values (the mean value of the residuals is, in fact, zero). The residuals Q-Q plot shows a pretty normal distribution. Overall, the evidence points toward the  model 1 being valid.


```{r}
lev<-hatvalues(model1)[hatvalues(model1)>2*(length(coef(model1))/length(hatvalues(model1)))]
out<-rstandard(model1)[abs(rstandard(model1)) > 2]
cd<-cooks.distance(model1)[cooks.distance(model1) >4/length(cooks.distance(model1))]
leve<-as.integer(names(lev))
oute<-as.integer(names(out))
cde<-as.integer(names(cd))
intersect(leve, oute)
intersect(oute,cde)
intersect(cde,leve)
intersect(intersect(cde,leve), oute)
influencePlot(model1, id.method="identify", main="InfluencePlot",sub="Circle size is proportional to Cookâ€™s distance")
plot(model1, which=5)
```

According to the analysis of the leverage, outlier and influential the obeservation 4,72,82,112,117 were removed.
```{r}
df2=df[-c( 4,72,82,112,117),]
model11=lm(Y~  x1 +  x3   +   x2   + I(x1^2)  +  x3*x2 ,data=df2)
summary(model11)

```
Final model is equal to : Y=12.83-0.815x1+0.81x1^2-0.032x2-0.72x3-0.0255x3*x2

For checking and interpreting the deregulation, we supposed that x2=15, so the model changes to Y=12.35-0.815x1+0.81x1^2-1.1025x3
```{r}
y0=12.35-0.815*df2$x1+0.81*df2$x1^2 
y1=12.35-0.815*df2$x1+0.81*df2$x1^2-1.1025
ggplot(df2, aes(x1)) +                    # basic graphical object
  geom_line(aes(y=y0,  colour="red"), linetype = "dashed") +  # first layer
  geom_line(aes(y=y1, colour="green")) + # second layer
  xlab("Distance") + 
  ylab("Total Cost") +
  ggtitle("Deregulation effect on cost when weight=15") +
  theme_bw(base_size=18) +
  scale_color_identity(name = "Type of deregulation",
                          breaks = c("red", "green"),
                          labels = c("Regulated", "Deregulated"),
                          guide = "legend")

```
Finally the above graph illustrated that because of the fact that red line (before deregulazation) is placed higher than green one (after deregulazation), we can concluse that deregulation was ables to decrease the total xost of transportation 


